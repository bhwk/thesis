\chapter{Methodology}
\section{Description of Pipeline}
Based on research into RAG vulnerabilities, there is a clear lack of security measures designed to preserve the privacy of a RAG corpus. This is especially important in fields like healthcare.
As demonstrated in \autocite{zeng2024goodbadexploringprivacy}, private information can be easily extracted by determined attackers through simple prompt injections.
Given that RAG relies on a set of documents as context and its vulnerabilities to RAG, we believe that generating a synthetic document separate from the corpus is sufficient to mitigate most issues.

\section{System Design}
As mentioned, the solution explored in this project consists of an agent-based document synthesis pipeline aimed at preventing raw LLM access to sensitive data.

For all intents and purposes, the pipeline operates in a similar fashion to typical RAG. Upon receiving a query, it fetches document from the RAG corpus then uses the retrieved documents as context in generating a response. However, we include an intermediary step between the information retrieval and inference steps.

Once the documents are retrieved, a  secondary LLM extracts only the necessary information from the documents retrieved. For instance, we may retrieve a medical record consisting of different medical readings for a query about a patient's blood pressure readings. In this example, we aim for the LLM to extract only the blood pressure readings from this document.

With the information retrieved, we use an agent-based approach to modify the information. In order to further distance the information from the original record, we apply the following steps.

Firstly, we remove any PII that may appear in the information. We consider the following as PII: names, ages, contact number and address. The LLM will remove, or replace with pseudonyms, any appearance of PII.

Secondly, we manipulate the data that appears in the information to generalise the record. Numbers are rounded, and converted to ranges if multiple readings of the same type occur.

Finally, to ensure that the LLM treats the synthesized information as relevant context, we modify the original query based on the synthetic information. It should be able to generate the same output as a model operating solely on RAG.

Once it has gone through this step, we pass the synthesized query and information to the primary LLM to generate a response.

Refer to figure \ref{fig:SynthLLMRAG} for a visualization of the system design.

\begin{figure}
	\includegraphics[width=\textwidth]{Synthesis LLM RAG example.png}
	\caption{System Design}
	\centering
	\label{fig:SynthLLMRAG}
\end{figure}

\section{Building the RAG Corpus}
RAG systems can make use of either structured or unstructured data, however in healthcare, data is usually structured.
In order to mimic real healthcare settings, we determined it was necessary to make use of data that was designed for real-world settings.
For our case, we will be making use of a synthetic Fast Healthcare Interoperability Resources (FHIR) dataset, generated and distributed by Synthea \autocite{Synthea2024}.

FHIR is a structured healthcare standard that defines how healthcare information can be shared between different systems regardless of how they are stored.
Individual FHIR patient records are stored in what is known as resources and each resource type represents specific information. A Patient resource would include the patient's name, date of birth, address, etc. Each resource type is specific to its use case.

FHIR records can appear in different file formats, JSON, XML, or RDF. For simpler parsing and handling, we will be making use of JSON FHIR files to build our RAG corpus.

We make use of the open-source library \textbf{Llamaindex} \autocite{Liu_LlamaIndex_2022} for abstractions when building the pipeline, as well as creating the database.

\subsection{FHIR Preprocessing}

First, we consider the type of data we wish to embed. JSON files are designed for programmatic use, meaning that they contain many identifying and delimiting tokens. If we were to convert the file in its entirety into its vector representation, it will result in detail being lost due to the repeated embedding of same key-value token pairs. Therefore, we first have to carry out flattening of the FHIR record.

Flattening the FHIR involves two things. First, we must determine what type of information we wish to extract. For this project, we are only working with information from the Patient resource, as well as the Observation, Procedure, Condition, Allergy and MedicationRequest resources. While initially the Encounter resource was used, we decided that it did not add any type of substantial information apart from the reason of the encounter as well as the location where it took place. Secondly, we have to convert the selected information into basic sentences. This is done by recursively un-nesting the FHIR resource with the information we specified.

The reason we do this is to improve the embedding accuracy of the FHIR record.
Firstly, we convert FHIR resources to basic sentences.
This is to avoid repeatedly embedding the same key-value token pairs and wasting embedding tokens.
Refer to figure \ref{fig:FHIRtoSentence} for an example.

\begin{figure}
	\includegraphics[width=\textwidth]{Converting FHIR to sentence.png}
	\caption{FHIR to sentence}
	\centering
	\label{fig:FHIRtoSentence}
\end{figure}

Processing the FHIR record, we group the information extracted from the Observation and Procedure resources by date. Afterwards, we collate the conditions, allergies, as well as the medications that has been assigned to the patient previously.

Using the archive officially distributed by \textbf{Synthea}, which contains 101 synthetic patients, we end up with a total of \textbf{5931} files. Figure \ref{fig:FileDistribution} shows the file distribution across patients. As we can see, there is a significant number of synthetic patients with over 100 files.

\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{file_distribution.png}
	\caption{File Distribution}
	\label{fig:FileDistribution}
\end{figure}

Over concerns about whether this will affect the retrieval results, we decided to remove any outliers that appear. How do we define outliers? By inspection we see that, on average, synthetic patients have around 40 to 60 files per patient, with few approaching 80 to 90 files.
Removing them, we end up with the following new distribution as seen in figure \ref{fig:FileDistributionAfter}. As we can see, we end up with a more reasonable distribution.

\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{file_distribution_trimmed.png}
	\caption{New File Distribution}
	\label{fig:FileDistributionAfter}
\end{figure}

Each of these documents are stored in separate files, marked by the patient's name followed by the date of the encounter to facilitate the document embedding process.

\subsection{Generating Embeddings and Keywords}

With the documents processed, we move on to embedding the documents. Vector embedding is the process of converting values to their vector equivalents, which are essentially a list of numbers representing our value.
By doing this, we can approximate the semantic similarity between objects through their proximity within the vector space. This makes it useful for RAG applications where semantic searches are common.

Before generating the vector embeddings from the documents, we have to consider which embedding model to use. Given that we are working with text, we looked towards text embedding models. Here the differences between models vary in size and performance. For this project we make use of open-source model \textbf{bge-base-en-v1.5} to generate the vector embeddings for the documents.

Each document is passed to the transformer model where it outputs the corresponding vector representation, then stored within the database. There are a number of different possible databases designed for vector embeddings, however, in this project we make use of the \textbf{Postgresql} database as well as the \textbf{pgvector} extension in order to enable vector storage.

In the process of generating the embeddings, we also generate a list of keywords that appear in the text document, storing them in separate indexes. This is to facilitate both keyword and semantic searches in the RAG system.

The overall process of the embedding generation is outlined in figure \ref{fig:EmbeddingsDatabase}.

\begin{figure}
	\includegraphics[width=\textwidth]{Store embeddings in DB.png}
	\caption{Embeddings to Database}
	\centering
	\label{fig:EmbeddingsDatabase}
\end{figure}

\subsubsection{Generating Questions}

After the process of creating the corpus, we also generate a single question based on the contents of each file. This is to test the accuracy of the RAG retrieval later.

Each file is passed to a LLM with the following prompt: \textit{"Generate a single question about the following text. Avoid general queries such as marriage status, death, age, contact, address. Text: $\{text\}$"}, and once the questions have been processed we store them in a JSON file for use later.

In general each question contains a piece of information contained within the text file, as well as the date associated.
\section{Large Language Model Choice}


With our RAG corpus processed and populated, we now look towards selecting the LLM to serve as the basis of our agents.
Not all LLMs are made equal, and a general convention is that the larger, the more sophisticated its response.
The intuitive choice is to make use of a well-established third-party LLM like OpenAI's ChatGPT, however, this is not acceptable due to the fact that it is a closed, proprietary model. In a field like healthcare, where private and sensitive information will be distributed, this is non-negotiable since it cannot be guaranteed that the data shared with ChatGPT will not be used for training or other purposes.

Given this reasoning, we look towards open-source LLMs. For our requirements, the LLM has to be able to make use of tools. Tools are functions that the LLM can call to perform a specific action. For example, it could call a function to add or subtract two numbers. In this project, functions are used to allow the LLM to perform searches on our RAG corpus.

There are a few notable LLMs that can make use of tools, such as Alibaba's reasoning model \textbf{QwQ}, Meta's \textbf{Llama3}, and Mistral AI's \textbf{Mistral}. In this project we decided to make use of Alibaba's \textbf{Qwen-2.5-32B}. It is a decent baseline model that performs adequately in all aspects, and we do not require the capabilities of a reasoning model like \textbf{QwQ}, as that will add to the total inference time.

\section{Agent Workflow Design}

Now that we have decided on the model to use, we move to designing the agents that will be involved in the workflow.

The document synthesis pipeline consists of three agents, each with their respective prompts.
There is the Synthesis Agent, which is in charge of transforming the information that is retrieved. The Search Agent is in charge of carrying out the necessary semantic searches on the database using the input query. Finally, the Review Agent is in charge of checking the synthesized information generated by the Synthesis Agent.

%TODO: Add the agent prompts to the appendix
The prompts for each agent are available in the appendix.

Figure \ref{fig:AgentInteraction} outlines the flow of interactions between the different agents.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{agent-workflow.png}
	\caption{Agent Interaction Flow}
	\label{fig:AgentInteraction}
\end{figure}

Upon receiving an input query, the Synthesis Agent passes off control to the Search Agent, which determines what kind of searches need to be run.

There are two ways the Search Agent can carry out searches. First, it can carry out a semantic search using information from the query. Secondly, it can check for patients that have been diagnosed with a specific condition. The logic behind this is to enable the Search Agent to carry out general queries, such as searching for medications given to patients diagnosed with the same condition.

At this stage, the Search Agent is also in charge of determining the right information to return. Here we make use of \textbf{Llamaindex's} implementation,  which consolidates smaller chunks into larger chunks that better fit the context window, refining the chunks sequantially untill all the chunks have been consumed.

Finally, once the necessary information has been consolidated, we return it to the Synthesis Agent.

The Synthesis Agent is in charge of transforming the information received. The main aspects that it changes is that it replaces names with pseudonyms, as well as attempts to remove all occurrence of PII within the retrieved information. Finally, it rounds off any numerical values that appear, and attempts to consolidate them into a range.

Using the newly synthesized information, it generates a new query in order to ensure that the LLM notices the relation between the synthesized context and query. The synthetic query may or may not differ from the original query, depending on the type of information retrieved, however the model should not produce a response that is too different from the baseline response.

The Review Agent's purpose is to ensure that the Synthesis Agent's response adheres to the guidelines set. It has been prompted similarly to the Synthesis Agent, however its main purpose is simply to point out any errors in the Synthesis Agent's response and allow for correction.

At the end of it, we will have a segment of text synthesized from the original documents that does not contain PII, with only the information that is needed to answer the input query.
\section{Search Functions}

% TODO: 
% Talk about the reasoning behind why we designed the way it is
% Talk about how we can vary the nodes that we retrieve
% Talk about why we used the workflow instead of just a plain function
With the workflow outlined, we move into designing the search functions for the Search Agent.

The retrieval method varies based on the type of information stored. In this project, we make use of semantic searches as well as keyword searches in our RAG system.

The semantic searches facilitate the retrieval of queries related to medical readings. This could be blood pressure, glucose, etc.

The keyword searches are targeted at looking up patients diagnosed with a specific condition. This is to facilitate two step searches. For example, an input query might ask \textit{"What medications are diabetes patients on?"}
If the agent only has access to semantic searches, it will not have enough information to answer the query unless we adjust it with a high \textit{k} value, but that does not guarantee that the nodes retrieved will be correct, because the search query will also be adjusted by the agent.

The process of semantic retrieval involves converting the input query into its vector equivalent in order to compare it to the other document vectors in the database. The similarity between vectors is computed using cosine similarity, with the formula defined as:
\[
	\text{Cosine Similarity} = \cos(\theta) = \frac{\mathbf{A} \cdot \mathbf{B}}{\|\mathbf{A}\| \|\mathbf{B}\|}
\]
The formula outputs a value between 0.0 to 1.0, representing no similarity and an exact match respectively, and returns the top \textit{k} results, where \textit{k} is an adjustable variable.
It is possible to set a minimum cut-off point for cosine similarity to adjust the relevance of the returned information, however we do not make use of this in the project.

To allow the agent to make use of both semantic and keyword searches, we turn the retrieval process into its own workflow, that takes a search mode as well as the query.

Depending on the mode of retrieval, we run the query against either our vector index or keyword index, then extract the necessary context information as well as the nodes that were retrieved before finally returning the result from our RAG workflow.

This is visually represented in figure \ref{fig:RagWorkflow}.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{rag-workflow.png}
	\caption{RAG workflow}
	\label{fig:RagWorkflow}
\end{figure}

With the RAG workflow defined, we create two separate functions that call the workflow with the \textit{'semantic'} and \textit{'keyword'} parameters and the input query. The input query can either be passed verbatim by the agent, or modified depending on the circumstances. These functions return two things, the retrieved information as well as the set of nodes that were retrieved. Refer to figures \ref{fig:SemanticSearchFunc} and
\ref{fig:KeywordSearch} for the code used.
\begin{figure}
	\centering
	\small
	\begin{lstlisting}[language=Python, breaklines=true]
async def retrieve_medical_readings_for_patient(
    query: str,
):
    """A tool for running semantic search for information related to a patient.
    Only contains patient information on a local database.
    Information consists of medical observations.
    Necessary to specify the patient's name in the form ([information to search] for [patient name])."""

    result = await rag.run(
        query=query,
        mode="semantic",
        vector_index=VECTOR_INDEX,
        keyword_index=KEYWORD_INDEX,
        llm=llm,
    )
    return result
\end{lstlisting}
	\caption{Semantic Search Function}
	\label{fig:SemanticSearchFunc}
\end{figure}

\begin{figure}
	\centering
	\small
	\begin{lstlisting}[language=Python, breaklines=true]
async def search_for_patients_with_medical_condition(
    query: str,
):
    """A tool to search for patients with the specified medical condition."""
    result = await rag.run(
        query=query,
        mode="keyword",
        vector_index=VECTOR_INDEX,
        keyword_index=KEYWORD_INDEX,
        llm=llm,
    )
    return result
\end{lstlisting}
	\caption{Keyword Search Function}
	\label{fig:KeywordSearch}
\end{figure}
